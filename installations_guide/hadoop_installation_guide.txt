###Before installing the hadoop make sure you have installed java on same machine.

Install hadooop on macos commands 

install the hadooop file on machine from link https://hadoop.apache.org/release/3.2.4.html

create a hadoop directory to install it 
mkdir -p /usr/local/opt/hadoop/


now move the downloaded hadoop file to 
mv /Users/piyush.dixit/Downloads/hadoop-3.3.4.tar.gz  /usr/local/opt/hadoop/

extract the tar file now using command 
tar -xvf hadoop-3.3.4.tar.gz


now add following commands in bashrc / zshrc 
nano ~/.zshrc  

# Hadoop
export HADOOP_HOME=/usr/local/opt/hadoop/hadoop-3.3.4
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

source ~/.zshrc


now make changes in few files 

change the directory
$HADOOP_HOME/etc/hadoop/

now edit files 
***********
nano core-site.xml

<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>

***********

nano hdfs-site.xml

***********

<configuration>
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>file:///usr/local/opt/hadoop/hadoop-3.3.4/data/namenode</value>
</property>

<property>
  <name>dfs.datanode.data.dir</name>
  <value>file:///usr/local/opt/hadoop/hadoop-3.3.4/data/datanode</value>
</property>
</configuration>

***********

nano yarn-site.xml

***********


<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>

***********


nano hdfs-site.xml

***********

<configuration>
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>file:///usr/local/opt/hadoop/hadoop-3.3.4/data/namenode</value>
</property>

<property>
  <name>dfs.datanode.data.dir</name>
  <value>file:///usr/local/opt/hadoop/hadoop-3.3.4/data/datanode</value>
</property>
</configuration>


***********



Now sudo systemsetup -setremotelogin on

Generate a key and authorize it for localhost
ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys

First connect will ask to trust localhost; say "yes"
ssh localhost 'echo ssh-ok'Enable passwordless SSH (required by start-dfs.sh)



now add add jvm (java istalled path ) at the bottom of the file 
nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
in my case i have added command 
export JAVA_HOME=/usr/local/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home



now change the directory 

cd /usr/local/opt/hadoop/hadoop-3.3.4/sbin


After installation check installtion use following behavious working or not 
Format NameNode (only first time)

hdfs namenode -format



Start HDFS (NameNode + DataNode)
start-dfs.sh
Start YARN (ResourceManager + NodeManager)
start-yarn.sh

Check Running Hadoop Processes
jps


Should list:
NameNode
DataNode
SecondaryNameNode
ResourceManager
NodeManager


Stop Hadoop Services
stop-yarn.sh
stop-dfs.sh
